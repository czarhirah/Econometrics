{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52520d4-f578-41a1-be75-ec15dfe10a6a",
   "metadata": {},
   "source": [
    "## Multivariate LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd09c2-a485-4967-babf-170de0717b30",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3fcc1c-0d62-4062-be1c-7151cb50b1b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_datareader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m wb\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVR\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_datareader'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "from pandas_datareader import data as wb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set figure parameters\n",
    "plt.rcParams['figure.figsize'] = [10, 7.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cab1b-daa8-440c-8920-99cf44347327",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd005eae-8a35-42d3-b38d-de13f7e422da",
   "metadata": {},
   "source": [
    "## FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b48c5-ea85-46c9-bd8e-65065c05becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.pdr_override()\n",
    "\n",
    "data = wb.get_data_yahoo('VND=x', start = '2003-01-01', end = '2024-12-31', interval = '1mo')\n",
    "\n",
    "# Obtain latest vnd to usd rate\n",
    "df_conv = wb.get_data_yahoo('VND=x', start = '2003-01-01', end = '2024-12-31')\n",
    "data1 = 1/df_conv\n",
    "vnd_to_usd = data1.iloc[-1,0]\n",
    "\n",
    "# Clean up statistics \n",
    "df = pd.DataFrame(data['Adj Close'])\n",
    "df.rename(columns = {'Adj Close':'USDVND'}, inplace = True)\n",
    "\n",
    "# change the datetime format\n",
    "usdvnd_up = pd.DataFrame(df['USDVND'].resample('YS').mean())\n",
    "usdvnd_up.index = pd.to_datetime(usdvnd_up.index, format = '%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1b6be-edef-4466-a465-a1dbf60e7803",
   "metadata": {},
   "source": [
    "## IMF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deffd7-f9eb-4f15-bd08-3bbbdb21ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imf_data(endpoint):\n",
    "    url = f\"https://www.imf.org/external/datamapper/api/v1/{endpoint}/VNM\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        values = data['values'][endpoint]['VNM']\n",
    "        df = pd.DataFrame(list(values.items()), columns=['Year', endpoint.replace('/', ' ')])\n",
    "        df['Year'] = pd.to_numeric(df['Year'])\n",
    "        df = df[(df['Year'] >= 2003) & (df['Year'] <= 2024)]\n",
    "        df.set_index('Year', inplace=True)\n",
    "        return df\n",
    "\n",
    "# Define endpoints for each indicator\n",
    "endpoints = {'NGDPDPC': 'GDP per Capita (Current Prices)',\n",
    "             'BCA': 'Current Account Balance',\n",
    "             'PCPIPCH': 'Average CPI'}\n",
    "\n",
    "# Fetch data for each endpoint and concatenate DataFrames\n",
    "dfs = {title: imf_data(endpoint) for endpoint, title in endpoints.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e672dcc-9164-4c2d-8acc-c76376816ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "imfdata = pd.concat(dfs.values(), axis=1)\n",
    "imfdata.columns = endpoints.values()\n",
    "imfdata.index = pd.to_datetime(imfdata.index, format = '%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d560c7c-a94b-4933-8dfd-00a201acf067",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fabd9-cff2-4791-8e3d-110cfceb251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(usdvnd_up,imfdata, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c8686-2c8e-4463-8a08-15d0e6356880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample the data using linear interp\n",
    "all_data = all_data.resample('MS').asfreq().interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bfd64-ca33-4bd5-a47d-278f742ba906",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = all_data.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07387b4-082b-4ab7-b1f9-4293ddcd06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# separate into train and test data\n",
    "train_size = int(len(dataset) * 0.66)\n",
    "test_size = len(dataset) - train_size \n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35230f1-182a-4322-9e9f-0aafa1c51743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back= 1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 5\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1566c9d-cee8-4055-847c-99270d8890a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "n = 4 # number of columns\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, n))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b92337-1a46-42a7-9120-dd87c31d808f",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15121dd0-ea09-4a1a-922c-714f1fc2397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(look_back, n)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=300, batch_size=16, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_lstm_model.h5')\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "trainPredict = np.squeeze(trainPredict)\n",
    "testPredict = np.squeeze(testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99770009-68af-47cd-aab4-b232c6105d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "def inverse_transform(arr):\n",
    "    extended = np.zeros((len(arr), n))\n",
    "    extended[:, 0] = arr\n",
    "    return scaler.inverse_transform(extended)[:, 0]\n",
    "\n",
    "\n",
    "trainPredict = inverse_transform(trainPredict)\n",
    "testPredict = inverse_transform(testPredict)\n",
    "trainY = inverse_transform(trainY)\n",
    "testY = inverse_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ecd5f-5a6f-4b4f-bb77-7b6a1f76c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift predictions up by one\n",
    "testPredict = np.delete(testPredict, -1)\n",
    "testY = np.delete(testY, 0)\n",
    "\n",
    "to_row = len(all_data) - len(testY)\n",
    "date_range = all_data[to_row:].index\n",
    "plt.plot(date_range, testPredict, color = 'blue', marker = 'o', linestyle = 'dashed', label = 'Predicted')\n",
    "plt.plot(date_range, testY, color = 'red', label = 'Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "testScore = testScore*vnd_to_usd\n",
    "\n",
    "print('Test Score: %.6f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a527b0d-03be-4036-9207-37cd477654da",
   "metadata": {},
   "source": [
    "# Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1c8af-dc34-49ac-9d86-c28f1dbf928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def generate_forecasts(model_path, test_data, forecast_steps):\n",
    "    \"\"\"\n",
    "    Generate forecasts beyond the test data using a trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "    - model_path: Path to the trained LSTM model file.\n",
    "    - test_data: Test data used for model evaluation.\n",
    "    - forecast_steps: Number of future time steps to forecast.\n",
    "    \n",
    "    Returns:\n",
    "    - forecasts: Array containing the forecasted values.\n",
    "    \"\"\"\n",
    "    # Load the trained LSTM model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Prepare the test data for forecasting\n",
    "    current_data = np.copy(test_data)\n",
    "    \n",
    "    # Generate forecasts\n",
    "    forecasts = []\n",
    "    for _ in range(forecast_steps):\n",
    "        # Predict the next time step\n",
    "        next_step_prediction = model.predict(current_data.reshape(1, *current_data.shape))\n",
    "        \n",
    "        # Append the prediction to the forecasts\n",
    "        forecasts.append(next_step_prediction[0, 0])\n",
    "        \n",
    "        # Update current data by removing the oldest time step and appending the latest prediction\n",
    "        current_data = np.roll(current_data, -1)\n",
    "        current_data[-1] = next_step_prediction\n",
    "    \n",
    "    return np.array(forecasts)\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'trained_lstm_model.h5' with the path to your trained LSTM model file\n",
    "# Replace 'test_data' with your actual test data\n",
    "# Replace 'forecast_steps' with the number of future time steps to forecast\n",
    "# forecasts = generate_forecasts('trained_lstm_model.h5', test_data, forecast_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3faf1b-4143-4772-b22d-f88aa827aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = testX[-1]\n",
    "forecast_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086d064-cd90-4eeb-8ee2-29c907e70277",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = generate_forecasts('trained_lstm_model.h5', test_data, forecast_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6ffac-9aef-4a47-8fed-3ffd228305b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = np.squeeze(forecasts)\n",
    "\n",
    "# Transform data\n",
    "def inverse_transform(arr):\n",
    "    extended = np.zeros((len(arr), n))\n",
    "    extended[:, 0] = arr\n",
    "    return scaler.inverse_transform(extended)[:, 0]\n",
    "\n",
    "forecasts = inverse_transform(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546f43b-3694-4177-a143-bd99de19ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "future_dates = [all_data.index[-1] + DateOffset(months = x) for x in range (0,forecast_steps+1)]\n",
    "future_dates_df = pd.DataFrame(index = future_dates[1:], columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5c00a-c38b-4118-a5e8-1a1336b23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(date_range, testPredict, color = 'blue', marker = 'o', linestyle = 'dashed', label = 'Validation')\n",
    "plt.plot(date_range, testY, color = 'red', label = 'Actual')\n",
    "plt.plot(future_dates_df.index, forecasts, color = 'green', marker = 'o', linestyle = 'dashed', label = 'Forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adbc57-aed9-4490-81db-f008ecdffb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
